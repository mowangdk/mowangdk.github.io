---
layout: post
title:  "weekly report"
date:   2023-01-15 22:30:08 +0800
categories: weeklyreport
---

快一个月没写总结了. 有点惭愧

# 读书

### 大话存储

本周无进展

# 论文

### It’s Time to Replace TCP in the Datacenter

[url](https://arxiv.org/pdf/2210.00714.pdf)

整理一下
TCP 在数据中心内的缺点
- 面向流的设计
  - 因为是流式发送数据, 数据在 tcp 这一层是没有所谓的"边界"的, 需要应用层在上层来确定数据的边界(对数据进行分割), 这就导致了额外的 payload(需要标记数据边界) 和逻辑处理(应用/tcp 包对数据边界进行处理) 
  - 这个流式对数据分发也特别的不友好, 因为一般 server 端是用多线程来处理用户请求的. 如果多个线程都从一个流中读取, 那么每个线程拿到的数据都有可能是不完整的. 当然我们可以新建一个线程从其他线程中收集信息. 但是这样成本太高,不建议. 所以现在一般都是一个线程负责从 socket 接受数据, 并将数据组成数据包然后分发给各个的处理线程(发出去的是数据包, 但是接收到的是字节流, 显然对于datacenter不太合适)
  - 论文中说, 以后会将一些逻辑下发到 NIC 中做, 现在的确是一种趋势, 如果 TCP 将包处理的基本逻辑放到 NIC 的话它势必不能依赖应用程序(对包的分割). 但是这种事情只是在特殊情况下才会出现,私以为不能作为一个正常理由来废弃 tcp. 。
- 面向链接的设计
  - 因为是面向链接的设计, tcp 的状态维护要花费很大的存储空间维持链接, 在数据通信量非常大的数据中心里面. 这点的确是很不利的. 例如 Linux Kernel 对于每一个 socket 都使用了 2000 bytes 去保存相关链接状态, 比如 packet buffer & additional state
- 共享带宽的设计
  - 在 tcp 里面, 如果遇到了主机链接过载, tcp 会将空闲的带宽平均分配给所有的现有链接. 这个叫 "fair scheduling".  但是这种策略的表现并不好.
  因为在 tcp 里面数据必须按照顺序传递, 非顺序的数据会被丢弃重传, 并且带宽底很容易造成长尾问题.会更加恶化当前的网络情况
- 发送拥塞控制设计
  - tcp 会根据网络的拥挤程度来降低数据包的发送速率, 但是发送端其实是很难感知到链路的拥塞程度的. 发送端主要依赖于缓存占用的拥塞信号来判断.但是这些都不是一手的
  - tcp 没有根据信息的优先级(发送到不同的优先队列)进行拥塞控制
基本上就是上面这些. 其他就是介绍了现在现存的一些以 tcp 协议为基础指定的一些协议, 和一些完全新创建的协议, 尽管说的这些协议各有各的好处, 不过我认为短期内还是没那么容易替换的, 尤其是完全新建的一些协议, 要数据中心内部所有的应用都使用多个协议来通讯这个有很大的实现难度, 并且新协议依旧没有经过长时间的验证. 大家还是喜欢熟悉的. 新的协议试错成本太高了



# 工作


### 磁盘容量占用问题

这周帮客户看一个问题. 又出现了一个 df & du 资源占用不一致的问题. 但是奇怪的点是, 进入到容器内没有找到对应的 deleted file. 宿主机上还是可以看到的. 一开始比较懵B. 不过详细看了下用户的应用就知道了, 用户有多个 container 同时使用了一个存储设备, 这样我在其中的一个 container 里面只能看到自己进程的引用. 其他进程的创建&销毁的文件当然我们是看不到的.
